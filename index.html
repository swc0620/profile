<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Woo Chul (Woochul) Shin</title>
  <meta name="author" content="Woochul Shin">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-J459KM922G"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-J459KM922G');
</script>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Woo Chul (Woochul) Shin
                  </p>
                  <p>Hello! I'm Woo Chul, a master's student majoring in CS at
                    <a href="https://www.cc.gatech.edu/">Georgia Tech</a>,
                    where I am advised by Prof. <a href="https://faculty.cc.gatech.edu/~danfei/">Danfei Xu</a>.
                  </p>
                  <p>
                    I earned my undergraduate degree at <a href="https://en.snu.ac.kr/">Seoul National University</a> where I double majored in Artificial Intelligence and Economics.
                  </p>
                  <p>
                    Feel free to drop me an email if you want to chat!
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:wshin49@gatech.edu">Email</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=dToGVDIAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/woochulshin/">LinkedIn
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/me.jpg" class="hoverZoomLink" />
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I study <b>1) robot learning</b> for robots to acquire and generalize skills from human videos and demonstrations. My research focuses on <b>2) dexterous manipulation with tactile and force feedback</b>, making robots adapt to contact-rich interactions in dynamic environments. I want to develop <b>3) robot reasoning</b> systems that plan, compose, and adapt skills over long horizons, moving beyond imitation toward purposeful autonomy. Ultimately, my goal is to build robots that can learn, think, and interact with humans in real-world environments.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='r2r_image'>
                    <img src='images/egoengine.png' width=100%>
                  </div>
                </div>
                <!-- <script type="text/javascript">
                  function r2r_start() {
                    document.getElementById('r2r_image').style.opacity = "1";
                  }
                  function r2r_stop() {
                    document.getElementById('r2r_image').style.opacity = "0";
                  }
                  r2r_stop()
                </script> -->
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle"><a href="https://egoengine.github.io/">EgoEngine: From Egocentric Human Videos to High-Fidelity Dexterous Robot Demonstrations</a></span>
                <br>
                <i>Yangcen Liu*</i>,
                <i>Shuo Cheng*</i>,
                <i>Yiran Yang</i>,
                <i>Xinchen Yin</i>,
                <strong>Woo Chul Shin</strong>, 
                <i>Mengying Lin</i>,
                <i>Danfei Xu</i>
                <br>
                <em>In Progress</em>
                <br>
                <a href="https://egoengine.github.io/">Project Website</a>
                <p></p>
                <p>
                  EgoEngine is a powerful data engine that converts egocentric human videos into robot-executable demonstrations. By coupling an action branch (retargeting with refinforcement learning refinement) and a visual branch (arm-hand inpainting with robot rendering blending) under the same task, EgoEngine minimizes action and visual gaps to yield scalable, executable robot data.
                </p>
              </td>
            </tr>

            <tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='r2r_image'>
                    <img src='images/compositional_visual_planning.png' width=100%>
                  </div>
                </div>
                <!-- <script type="text/javascript">
                  function r2r_start() {
                    document.getElementById('r2r_image').style.opacity = "1";
                  }
                  function r2r_stop() {
                    document.getElementById('r2r_image').style.opacity = "0";
                  }
                  r2r_stop()
                </script> -->
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle"><a href="https://comp-visual-planning.github.io/">Compositional Visual Planning via Inference-Time Diffusion Scaling</a></span>
                <br>
                <i>Yixin Zhang</i>,
                <i>Yunhao Luo</i>,
                <i>Utkarsh Aashu Mishra</i>,
                <strong>Woo Chul Shin</strong>, 
                <i>Yongxin Chen</i>,
                <i>Danfei Xu</i>
                <br>
                <em>ICLR 2026</em>
                <br>
                <a href="https://openreview.net/pdf?id=EEONns7ae4">Paper</a>
                /
                <a href="https://comp-visual-planning.github.io/">Project Website</a>
                <p></p>
                <p>
                  Compositional Visual Planning via Inference-Time Diffusion Scaling is a training-free framework that extends diffusion models from short-horizon to long-horizon robot planning. Instead of averaging independently denoised segments—which leads to instability in noisy spaces—it enforces boundary consistency on Tweedie estimates through a combination of synchronous and asynchronous message passing across a chain-structured factor graph. This inference-time approach produces globally coherent plans and significantly improves generalization to unseen start–goal tasks across diverse simulation environments.
                </p>
              </td>
            </tr>
            
            <tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='r2r_image'><img src='images/immimic.png' width=100%>
                  </div>
                </div>
                <!-- <script type="text/javascript">
                  function r2r_start() {
                    document.getElementById('r2r_image').style.opacity = "1";
                  }
                  function r2r_stop() {
                    document.getElementById('r2r_image').style.opacity = "0";
                  }
                  r2r_stop()
                </script> -->
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle"><a href="https://sites.google.com/view/immimic">ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation</a></span>
                <br>
                <i>Yangcen Liu*</i>,
                <strong>Woo Chul Shin*</strong>,
                <i>Yunhai Han</i>,
                <i>Zhenyang Chen</i>,
                <i>Harish Ravichandar</i>,
                <i>Danfei Xu</i>,
                <br>
                <em>CoRL 2025 <b>(Oral)</b></em>
                <br>
                <em>CoRL H2R Workshop 2025 </em>
                <br>
                <em>RSS Dex Workshop 2025 <b>(Spotlight)</b></em>
                <br>
                <a href="https://www.youtube.com/live/rh2oxU1MCb0?si=IIKzoSivMLy-9NKp&t=2480">Oral Talk</a>
                /
                <a href="https://arxiv.org/pdf/2509.10952">Paper</a>
                /
                <a href="https://sites.google.com/view/immimic">Project Website</a>
                /
                <a href="https://github.com/GaTech-RL2/ImMimic-CoRL2025">Code</a>
                <p></p>
                <p>
                  ImMimic is an embodiment-agnostic co-training framework that bridges the domain gap between large-scale human videos and limited robot demonstrations. It uses Dynamic Time Warping to map human hand poses to robot joints and applies MixUp interpolation to generate intermediate domains for smoother domain adaptation. Evaluations across two parallel-jaw grippers (Robotiq, Fin Ray) and two dexterous hands (Ability, Allegro) demonstrate that ImMimic significantly improves task success and execution smoothness.
                </p>
              </td>
            </tr>


            <tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='r2r_image'><video  width=100% muted autoplay loop>
                    <source src="images/sail.mp4" type="video/mp4">
                      Your browser does not support the video tag.
                    </video>
                  </div>
                </div>
                <!-- <script type="text/javascript">
                  function r2r_start() {
                    document.getElementById('r2r_image').style.opacity = "1";
                  }
                  function r2r_stop() {
                    document.getElementById('r2r_image').style.opacity = "0";
                  }
                  r2r_stop()
                </script> -->
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle"><a href="https://nadunranawaka1.github.io/sail-policy/">SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies</a></span>
                <br>
                <i>Nadun Ranawaka Arachchige*</i>,
                <i>Zhenyang Chen*</i>,
                <i>Wonsuhk Jung</i>,
                <strong>Woo Chul Shin</strong>, 
                <i>Rohan Bansal</i>,
                <i>Pierre Barroso</i>,
                <i>Yu Hang He</i>,
                <i>Yingyan Celine Lin</i>,
                <i>Benjamin Joffe</i>,
                <i>Shreyas Kousik<sup>&dagger;</sup></i>,
                <i>Danfei Xu<sup>&dagger;</sup></i>
                <br>
                <em>CoRL 2025 <b>(Oral)</b></em>
                <br>
                <a href="https://arxiv.org/abs/2506.11948">Paper</a>
                /
                <a href="https://nadunranawaka1.github.io/sail-policy/">Project Website</a>
                <p></p>
                <p>
                  SAIL (Speed-Adaptive Imitation Learning) is a framework for enabling faster-than- demonstration execution of policies by addressing key technical challenges in robot dynamics and state-action distribution shifts. SAIL achieves up to a 4× speedup over demonstration speed in simulation and up to 3.2× speedup on physical robot.
                </p>
              </td>
            </tr>
            

            <tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <!-- <div class="two" id='r2r_image'><video  width=100% muted autoplay loop>
                    <source src="images/r2r.mp4" type="video/mp4">
                      Your browser does not support the video tag.
                    </video>
                  </div> -->
                  <img src='images/mimiclabs.jpg' width=100%>
                </div>
                <!-- <script type="text/javascript">
                  function r2r_start() {
                    document.getElementById('r2r_image').style.opacity = "1";
                  }
                  function r2r_stop() {
                    document.getElementById('r2r_image').style.opacity = "0";
                  }
                  r2r_stop()
                </script> -->
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle"><a href="https://robo-mimiclabs.github.io/pages/study.html">What Matters in Learning from Large-Scale Datasets for Robot Manipulation</a></span>
                <br>
                <i>Vaibhav Saxena</i>,
                <i>Matthew Bronars*</i>,
                <i>Nadun Ranawaka Arachchige*</i>,
                <i>Kuancheng Wang</i>,
                <strong>Woo Chul Shin</strong>, 
                <i>Soroush Nasiriany</i>,
                <i>Ajay Mandlekar<sup>&dagger;</sup></i>,
                <i>Danfei Xu<sup>&dagger;</sup></i>
                <br>
                <em>ICLR 2025</em>
                <br>
                <a href="https://arxiv.org/pdf/2506.13536">Paper</a>
                /
                <a href="https://robo-mimiclabs.github.io/pages/study.html">Project Website</a>
                /
                <a href="https://github.com/Gatech-RL2/mimiclabs">Code</a>
                /
                <a href="https://huggingface.co/datasets/vaibhavsaxena11/mimiclabs_datasets">Data</a>
                <p></p>
                <p>
                  MimicLabs is a data generation framework to procedurally emulate key sources of diversity in robot datasets. Using this framework, we generate large-scale datasets with controlled variations to analyze how collection diversity and retrieval strategies impact downstream policy learning.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Work Experience</h2>
                <p>
                  <strong>Software Development Engineer Intern, Amazon</strong> <i> (May 2025 -- Aug. 2025)</i>
                  </br>
                  <i>Bellevue, WA</i>
                  </br>
                  * Developed AI Orchestration and Responsible AI components for Amazon Nova AI models.
                </p>
                <p>
                  <strong>Graduate Research Assistant, Georgia Tech</strong> <i> (Fall 2024 -- May 2026)</i>
                  </br>
                  <i>Atlanta, GA</i> / Lab: <a href="https://rl2.cc.gatech.edu/">Robot Learning and Reasoning Lab</a> / Advisor: <a href="https://faculty.cc.gatech.edu/~danfei/">Danfei Xu</a> 
                </p>
                
                <p>
                  <strong>Software Engineer, Kakao</strong> <i> (Jun. 2024 -- Jul. 2024)</i>
                  </br>
                  <i>South Korea</i>
                  </br>
                  * Developed an Kubernetes-based machine learning platform for AI model training and inference.
                </p>
                <p>
                  <strong>Software Engineer, Kakao Brain</strong> <i> (Aug. 2022 -- May. 2024)</i>
                  </br>
                  <i>South Korea</i>
                  </br>
                  * Developed an Kubernetes-based machine learning platform for AI model training and inference.
                </p>
                <p>
                  <strong>Software Engineer Intern, Elysia</strong> <i> (Mar. 2022 -- Jun. 2022)</i>
                  </br>
                  <i>South Korea</i>
                  </br>
                  * Built backend server and smart contracts for a Ethereum-based crowdfunding service.
                </p>
                <p>
                  <strong>Software Engineer Intern, Kakao Brain</strong> <i> (Dec. 2021 -- Mar. 2022)</i>
                  </br>
                  <i>South Korea</i>
                  </br>
                  * Built an AI resume creation service using GPT-2.
                </p>
                <p>
                  <strong>Software Engineer Intern, SNUAILAB</strong> <i> (Dec. 2020 -- Mar. 2021)</i>
                  </br>
                  <i>South Korea</i>
                  </br>
                  * Developed a YOLOv4-based image labeling system to annotate pig farm data.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Teaching Experience</h2>
                <p>
                  <strong>CS4644/CS7643, Deep Learning, Georgia Tech</strong>
                  </br>
                  Fall 2024, Spring 2025, Fall 2025, Spring 2026
                </p>
                
                <p>
                  <strong>M2177.007100, Field Application Research of Blockchain 2, SNU</strong>
                  </br>
                  Spring 2022
                </p>
                <p>
                  <strong>M2177.007000, Field Application Research of Blockchain 1, SNU</strong>
                  </br>
                  Fall 2021
                </p>
                <p>
                  <strong>L0444.000400, Basic Computing: First Adventures in Computing, SNU</strong>
                  </br>
                  Spring 2021, Summer 2021
                </p>
              </td>
            </tr>
          </tbody></table>

          <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Embodiment Experience</h2>
                RobotiQ 2F-85 Gripper
                </br>
                Fin Ray Gripper (UMI Gripper)
                </br>
                Psyonic Ability Hand
                </br>
                Allegro AH V4 Hand
                </br>
                Franka Emika Panda Arm
                </br>
                Rainbow Robotics RB-Y1
                <br>
                Robotera Xhand1
                <br>
                Mindrove Armband
              </td>
            </tr>
          </tbody></table> -->
        </td>
      </tr>
      <tr>
        <td>
          <p>
            <a href="https://jonbarron.info/">Template Source</a>
          </p>
        </td>
      </tr>
    </tbody>
  </table>
</body>

</html>