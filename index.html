<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Woo Chul (Woochul) Shin</title>
  <meta name="author" content="Woochul Shin">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-J459KM922G"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-J459KM922G');
</script>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Woo Chul (Woochul) Shin
                  </p>
                  <p>Hello! I'm Woo Chul, a master's student majoring in CS at
                    <a href="https://www.cc.gatech.edu/">Georgia Tech</a>,
                    where I am advised by Prof. <a href="https://faculty.cc.gatech.edu/~danfei/">Danfei Xu</a>.
                  </p>
                  <p>
                    I earned my undergraduate degree at <a href="https://en.snu.ac.kr/">Seoul National University</a> where I double majored in Artificial Intelligence and Economics.
                  </p>
                  <p>
                    Please feel free to check out my CV and drop me an email if you want to chat!
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:loopdewoo@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=dToGVDIAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://drive.google.com/file/d/1YLbdbqdEelH2K_OX1XqsX7pScckiqNka/view?usp=sharing">CV (Aug 2024)</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/woochulshin">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://github.com/swc0620/">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/me.jpg" class="hoverZoomLink" />
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in robotics, particularly in robot learning and dexterous manipulation. My research focuses on enabling robots to perform dexterous manipulation in dynamic and complex environments, learning to perceive and interact with the world effectively. A key aspect of my work involves learning from human video demonstrations to teach robots nuanced manipulation skills and intuitive interactions, bridging perception and action in real-world scenarios.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='r2r_image'><img src='images/immimic.png' width=100%>
                  </div>
                </div>
                <!-- <script type="text/javascript">
                  function r2r_start() {
                    document.getElementById('r2r_image').style.opacity = "1";
                  }
                  function r2r_stop() {
                    document.getElementById('r2r_image').style.opacity = "0";
                  }
                  r2r_stop()
                </script> -->
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle"><a href="https://sites.google.com/view/immimic">ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation</a></span>
                <br>
                <strong>Woo Chul Shin*</strong>,
                <i>Yangcen Liu*</i>,
                <i>Yunhai Han</i>,
                <i>Zhenyang Chen</i>,
                <i>Harish Ravichandar</i>,
                <i>Danfei Xu</i>,
                <br>
                <em>CoRL 2025 <b>(Oral)</b></em>
                <br>
                <em>RSS 2025 Dex WS <b>(Spotlight)</b></em>
                <br>
                <a href="https://openreview.net/pdf/71eeb1380524067a776d1c563ac1a37bf0f94f98.pdf">Paper</a>
                /
                <a href="https://sites.google.com/view/immimic">Project Website</a>
                <p></p>
                <p>
                  ImMimic is an embodiment-agnostic co-training framework that bridges the domain gap between large-scale human videos and limited robot demonstrations. It uses Dynamic Time Warping to map human hand poses to robot joints and applies MixUp interpolation to generate intermediate domains for smoother domain adaptation. Evaluations across two parallel-jaw grippers (Robotiq, Fin Ray) and two dexterous hands (Ability, Allegro) demonstrate that ImMimic significantly improves task success and execution smoothness.
                </p>
              </td>
            </tr>


            <tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='r2r_image'><video  width=100% muted autoplay loop>
                    <source src="images/sail.mp4" type="video/mp4">
                      Your browser does not support the video tag.
                    </video>
                  </div>
                </div>
                <!-- <script type="text/javascript">
                  function r2r_start() {
                    document.getElementById('r2r_image').style.opacity = "1";
                  }
                  function r2r_stop() {
                    document.getElementById('r2r_image').style.opacity = "0";
                  }
                  r2r_stop()
                </script> -->
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle"><a href="https://nadunranawaka1.github.io/sail-policy/">SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies</a></span>
                <br>
                <i>Nadun Ranawaka Arachchige*</i>,
                <i>Zhenyang Chen*</i>,
                <i>Wonsuhk Jung</i>,
                <strong>Woo Chul Shin</strong>, 
                <i>Rohan Bansal</i>,
                <i>Pierre Barroso</i>,
                <i>Yu Hang He</i>,
                <i>Yingyan Celine Lin</i>,
                <i>Benjamin Joffe</i>,
                <i>Shreyas Kousik<sup>&dagger;</sup></i>,
                <i>Danfei Xu<sup>&dagger;</sup></i>
                <br>
                <em>CoRL 2025 <b>(Oral)</b></em>
                <br>
                <a href="https://arxiv.org/abs/2506.11948">Paper</a>
                /
                <a href="https://nadunranawaka1.github.io/sail-policy/">Project Website</a>
                <p></p>
                <p>
                  SAIL (Speed-Adaptive Imitation Learning) is a framework for enabling faster-than- demonstration execution of policies by addressing key technical challenges in robot dynamics and state-action distribution shifts. SAIL achieves up to a 4× speedup over demonstration speed in simulation and up to 3.2× speedup on physical robot.
                </p>
              </td>
            </tr>
            

            <tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <!-- <div class="two" id='r2r_image'><video  width=100% muted autoplay loop>
                    <source src="images/r2r.mp4" type="video/mp4">
                      Your browser does not support the video tag.
                    </video>
                  </div> -->
                  <img src='images/mimiclabs.jpg' width=100%>
                </div>
                <!-- <script type="text/javascript">
                  function r2r_start() {
                    document.getElementById('r2r_image').style.opacity = "1";
                  }
                  function r2r_stop() {
                    document.getElementById('r2r_image').style.opacity = "0";
                  }
                  r2r_stop()
                </script> -->
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle"><a href="https://robo-mimiclabs.github.io/pages/study.html">What Matters in Learning from Large-Scale Datasets for Robot Manipulation</a></span>
                <br>
                <i>Vaibhav Saxena</i>,
                <i>Matthew Bronars*</i>,
                <i>Nadun Ranawaka Arachchige*</i>,
                <i>Kuancheng Wang</i>,
                <strong>Woo Chul Shin</strong>, 
                <i>Soroush Nasiriany</i>,
                <i>Ajay Mandlekar<sup>&dagger;</sup></i>,
                <i>Danfei Xu<sup>&dagger;</sup></i>
                <br>
                <em>ICLR 2025</em>
                <br>
                <a href="https://arxiv.org/pdf/2506.13536">Paper</a>
                /
                <a href="https://robo-mimiclabs.github.io/pages/study.html">Project Website</a>
                /
                <a href="https://github.com/Gatech-RL2/mimiclabs">Code</a>
                /
                <a href="https://huggingface.co/datasets/vaibhavsaxena11/mimiclabs_datasets">Data</a>
                <p></p>
                <p>
                  MimicLabs is a data generation framework to procedurally emulate key sources of diversity in robot datasets. Using this framework, we generate large-scale datasets with controlled variations to analyze how collection diversity and retrieval strategies impact downstream policy learning.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Work Experience</h2>
                <p>
                  <strong>Software Development Engineer Intern, Amazon</strong> <i> (May 2025 -- Aug. 2025)</i>
                  </br>
                  <i>Bellevue, WA</i>
                  </br>              
                  * Developed AI Orchestration and Responsible AI components for Amazon's Nova AI models.   
                </p>
                <p>
                  <strong>Research Assistant, Georgia Tech</strong> <i> (Fall 2024 -- Present)</i>
                  </br>
                  <i>Atlanta, GA</i>
                  </br>
                  Lab: <a href="https://rl2.cc.gatech.edu/">Robot Learning and Reasoning Lab</a>
                  </br>
                  Advisor: <a href="https://faculty.cc.gatech.edu/~danfei/">Danfei Xu</a>
                  </br>
                  Collaborator: <a href="https://y8han.github.io/" >Yunhai Han</a> from <a href="https://star-lab.cc.gatech.edu/" >STAR Lab</a>
                </p>
                
                <p>
                  <strong>Software Engineer, Kakao</strong> <i> (Jun. 2024 -- Jul. 2024)</i>
                  </br>
                  <i>South Korea</i>
                  </br>
                  * Developed an Kubernetes-based ML platform for AI model training and inference.                  
                </p>
                <p>
                  <strong>Software Engineer, Kakao Brain</strong> <i> (Aug. 2022 -- May. 2024)</i>
                  </br>
                  <i>South Korea</i>
                  </br>
                  * Developed an Kubernetes-based ML platform from scratch for AI model training and inference.
                </p>
                <p>
                  <strong>Software Engineer Intern, Elysia</strong> <i> (Mar. 2022 -- Jun. 2022)</i>
                  </br>
                  <i>South Korea</i>
                  </br>
                  * Built backend server and smart contracts for a blockchain-based crowdfunding service.              
                </p>
                <p>
                  <strong>Software Engineer Intern, Kakao Brain</strong> <i> (Dec. 2021 -- May. 2022)</i>
                  </br>
                  <i>South Korea</i>
                  </br>
                  * Built an AI resume creation service using GPT-2.                  
                </p>
                <p>
                  <strong>Software Engineer Intern, SNUAILAB</strong> <i> (Dec. 2020 -- May. 2021)</i>
                  </br>
                  <i>South Korea</i>
                  </br>
                  * Developed a YOLOv4-based image labeling system to annotate pig farm data.         
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Teaching Experience</h2>
                <p>
                  <strong>CS4644/CS7643, Deep Learning, Georgia Tech</strong>
                  </br>
                  Fall 2025, Graduate Teaching Assistant
                </p>
                <p>
                  <strong>CS4644/CS7643, Deep Learning, Georgia Tech</strong>
                  </br>
                  Spring 2025, Graduate Teaching Assistant
                </p>
                <p>
                  <strong>CS4644/CS7643, Deep Learning, Georgia Tech</strong>
                  </br>
                  Fall 2024, Graduate Teaching Assistant
                </p>
                <p>
                  <strong>M2177.007100, Field Application Research of Blockchain 2, SNU</strong>
                  </br>
                  Spring 2022, Teaching Assistant
                </p>
                <p>
                  <strong>M2177.007000, Field Application Research of Blockchain 1, SNU</strong>
                  </br>
                  Fall 2021, Teaching Assistant
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Embodiment Experience</h2>
                RobotiQ 2F-85 Gripper
                </br>
                Fin Ray Gripper
                </br>
                PSYONIC Ability Hand
                </br>
                Allegro AH V4 Hand
                </br>
                Franka Emika Panda Arm
                </br>
                Rainbow Robotics RB-Y1
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </tbody>
  </table>
</body>

</html>